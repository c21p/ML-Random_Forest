# -*- coding: utf-8 -*-
"""Assignment6_cp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QZYOl_lyHEAQq5NbIfmZDcS5g31DrXt3

**1. Gather and prepare data: Gather and prepare the data you will use to train the model. This may involve collecting data from various sources,
do EDA, cleaning and formatting the data, feature selection, encoding, and splitting it into training, validation, and testing sets.**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""**# Data Collection**"""

df = pd.read_csv("Campaign_data_statistics_section_1.csv")
df.head()

"""**# EDA**"""

df.info()

# Check null values
df.isna().sum()

df.nunique()

# campaign_id column is not so important we can drop it
# df.drop(['campaign_id'], axis=1, inplace=True)

plt.figure(figsize=(20,10))
sns.heatmap(df.corr(), annot=True, fmt=".2f", annot_kws = {'size' : 15}, linewidth = 5, linecolor = 'orange')
plt.show()

# from above we can say that is_timer column contains only zero value
# it is not so important we can drop it
df.drop(['is_timer'], axis=1, inplace=True)

df['times_of_day'].unique()

# Encode times_of_day column

df['times_of_day'].replace({"Noon":0,"Morning":1,"Evening":2}, inplace=True)

# Feature selection by linear regression
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import SelectFromModel

x = df.drop("click_rate",axis=1)
y = df['click_rate']

# linear regression
feature_select = SelectFromModel(LinearRegression())
feature_select.fit(x,y)
feature_0 = feature_select.get_feature_names_out()
print(f"Important features are: {feature_0}")

# Feature selection by RandomForestRegressor

from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel

# linear regression
feature_select = SelectFromModel(RandomForestRegressor())
feature_select.fit(x,y)
feature_1 = feature_select.get_feature_names_out()
print(f"Important features are: {feature_1}")

# Feature selection by DecisionTreeRegressor

from sklearn.tree import DecisionTreeRegressor

# linear regression
feature_select = SelectFromModel(DecisionTreeRegressor())
feature_select.fit(x,y)
feature_2 = feature_select.get_feature_names_out()
print(f"Important features are: {feature_2}")

# Train test split
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=42)

"""**2.Check distribution of the data, outliers and missing value treatment.**"""

# Distribution of data
import matplotlib.pyplot as plt
plt.figure(figsize=(20,20))
plot_num = 1
for i in df.columns:
    plt.subplot(8,3,plot_num)
    sns.histplot(df[i],kde=True)
    plot_num+=1
plt.show()

"""From above graph distribution we can say that data is not normaly distributed"""

int_column_list = x.select_dtypes(include= [int,float]).columns
# len(int_column_list)
plt.figure(figsize=(15,25))
x=1
for i in int_column_list:
    plt.subplot(6,4,x)
    sns.boxplot(df[i])
    x+=1
plt.show()

"""**3. Choose a model: Use Random Forest Algorithm.**"""

# Normalize the data
from sklearn.preprocessing import MinMaxScaler

normal_scaler = MinMaxScaler()
normal_scaler.fit(x_train)

train_array = normal_scaler.transform(x_train)
test_array = normal_scaler.transform(x_test)

model = RandomForestRegressor()

"""**4.	Train the model: You'll need to train it using the data you collected and prepared in step 2. This involves feeding the data into the model and adjusting the model's parameters until it produces accurate results.**"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

model.fit(train_array, y_train)
y_pred_train = model.predict(train_array)

mse = mean_squared_error(y_train,y_pred_train)
print("MSE :",mse)

rmse = np.sqrt(mse)
print("RMSE:",rmse)

mae = mean_absolute_error(y_train,y_pred_train)
print("MAE :",mae)

r2_value = r2_score(y_train,y_pred_train)
print("R-Squared :",r2_value)

"""**5.	Evaluate the model: After training the model, you'll need to evaluate its performance using the validation and testing sets. This will help you determine how well the model is able to generalize to new data and whether it is overfitting or underfitting.**"""

#Testing data accuracy
y_pred_test = model.predict(test_array)

mse = mean_squared_error(y_test,y_pred_test)
print("MSE :",mse)

rmse = np.sqrt(mse)
print("RMSE:",rmse)

mae = mean_absolute_error(y_test,y_pred_test)
print("MAE :",mae)

r2_value = r2_score(y_test,y_pred_test)
print("R-Squared :",r2_value)

"""from above results we can say that our model meets to overfitting

**6.	Fine-tune the model: Based on the results of the evaluation, you may need to fine-tune the model by adjusting the model's parameters, selecting different features, or using a different model architecture.**
"""

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomForestRegressor

# Define the parameter grid to search
param_grid = {'n_estimators': [100, 200, 300],
              'max_depth': [5, 10, 20],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4]}

# Initialize a random forest regressor
model = RandomForestRegressor()

# Initialize GridSearchCV to search the best hyperparameters
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)

# Fit the GridSearchCV to the data
grid_search.fit(train_array, y_train)

# Print the best hyperparameters
# print("Best hyperparameters: ", grid_search.best_params_)

grid_search.best_params_

# Training Performance
model_1 = grid_search.best_estimator_

model_1.fit(train_array, y_train)
y_pred_train = model_1.predict(train_array)

mse = mean_squared_error(y_train,y_pred_train)
print("MSE :",mse)

rmse = np.sqrt(mse)
print("RMSE:",rmse)

mae = mean_absolute_error(y_train,y_pred_train)
print("MAE :",mae)

r2_value = r2_score(y_train,y_pred_train)
print("R-Squared :",r2_value)

# Testing Performance
model_1 = grid_search.best_estimator_

y_pred_test = model_1.predict(test_array)

mse = mean_squared_error(y_test,y_pred_test)
print("MSE :",mse)

rmse = np.sqrt(mse)
print("RMSE:",rmse)

mae = mean_absolute_error(y_test,y_pred_test)
print("MAE :",mae)

r2_value = r2_score(y_test,y_pred_test)
print("R-Squared :",r2_value)

from sklearn.model_selection import RandomizedSearchCV


# Define the parameter grid to search
param_grid = {'n_estimators': [100, 200, 300],
              'max_depth': [5, 10, 20],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4],
              'n_jobs': [-1, 1, 2],
              'ccp_alpha': [0.0, 0.1, 0.01]}

# Initialize a random forest regressor
model = RandomForestRegressor()

# Initialize GridSearchCV to search the best hyperparameters
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=5)

# Fit the GridSearchCV to the data
random_search.fit(x_train, y_train)
random_search.best_params_

# Training Performance
model_2 = random_search.best_estimator_

model_2.fit(train_array, y_train)
y_pred_train = model_2.predict(train_array)

mse = mean_squared_error(y_train,y_pred_train)
print("MSE :",mse)

rmse = np.sqrt(mse)
print("RMSE:",rmse)

mae = mean_absolute_error(y_train,y_pred_train)
print("MAE :",mae)

r2_value = r2_score(y_train,y_pred_train)
print("R-Squared :",r2_value)

# Testing Performance

y_pred_test = model_2.predict(test_array)

mse = mean_squared_error(y_test,y_pred_test)
print("MSE :",mse)

rmse = np.sqrt(mse)
print("RMSE:",rmse)

mae = mean_absolute_error(y_test,y_pred_test)
print("MAE :",mae)

r2_value = r2_score(y_test,y_pred_test)
print("R-Squared :",r2_value)

